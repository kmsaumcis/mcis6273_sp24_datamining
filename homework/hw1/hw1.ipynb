{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["\\begin{center}\n", "\\begin{huge}\n", "MCIS6273 Data Mining (Prof. Maull) / Spring 2024 / HW1\n", "\\end{huge}\n", "\\end{center}\n", "\n", "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n", "|:---------------:|:--------:|:---------------:|\n", "| 20 | Wednesday February 28 @ Midnight | _up to_ 20 hours |\n", "\n", "\n", "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n", "\n", "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n", "\n", "## OBJECTIVES\n", "* Perfom basic data engineering and visualization in Python using an external set data.\n", "\n", "* Perform  data engineering in Python using OpenAQ API.\n", "\n", "* Perfom basic data analysis in Python using OpenAQ weather data.\n", "\n", "* Perfom basic statistical significance tests of air quality data.\n", "\n", "* Complete the online HW1 assessment.\n", "\n", "* BONUS: Build and analyze a more complex OpenAQ data set.\n", "\n", "## WHAT TO TURN IN\n", "You are being encouraged to turn the assignment in using the provided\n", "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n", "`homework/hw0`.   Put all of your files in that directory.  Then zip or tar that directory,\n", "rename it with your name as the first part of the filename (e.g. `maull_hw0_files.zip`, `maull_hw0_files.tar.gz`), then\n", "download it to your local machine, then upload the `.zip` to Blackboard.\n", "\n", "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n", "on the basics of using zip in Linux.\n", "\n", "If you choose not to use the provided notebook, you will still need to turn in a\n", "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n", "this homework.\n", "\n", "\n", "## ASSIGNMENT TASKS\n", "### (10%) Perfom basic data engineering and visualization in Python using an external set data. \n", "\n", "Like last homework, you will continue your practice of data engineering to\n", "prepare data for analysis.  \n", "\n", "This time, we will get exposure to gathering data from an API \n", "and visualizing that data.\n", "\n", "You are aware that there are many airports in the US -- and\n", "in this assignment, we will get the latitude and longitude\n", "coordinates of over 60 major airports.\n", "\n", "The power of open data cannot be over-emphasized in this \n", "part of the assignment, since without it, we would not \n", "be able to perform these actions so efficiently, but\n", "furthermore, we would not be able to get to \n", "the _important_ questions so quickly.\n", "\n", "This part is a warm up for what is to come.\n", "\n", "All of your code must be implemented in Jupyter as a notebook -- you\n", "will be required to turn in a `.ipynb` file.\n", "\n", "**&#167; Task:**  **Use Python to make HTTP/API calls to obtain and prepare data.**\n", "  \n", "\n", "The first order of business will require you to obtain the list\n", "of all US airports from this comprehensive site:\n", "\n", "\n", "- [https://data.humdata.org/dataset/ourairports-usa](https://data.humdata.org/dataset/ourairports-usa)\n", "\n", "The direct link to this file is here:\n", "\n", "- [https://ourairports.com/countries/US/airports.hxl](https://ourairports.com/countries/US/airports.hxl)\n", "\n", "and you can call `pd.read_csv()` directly on it to \n", "pull the data into a DataFrame!\n", "\n", "Perform the following:\n", "\n", "* store the entire DataFrame in a `csv` file and name it: `all_us_airports.csv`\n", "* make sure the data is clean -- that is you may notice some non useful data \n", "  when you call for the data -- there is one row in particular; remove that non useful row\n", "\n", "\n", "**&#167; Task:**  **Filter data to a subset for further use.**\n", "\n", "\n", "Now that you have a useful file, we will want to filter it further so \n", "we can restrict it to just the data we are interested in.\n", "\n", "Specifically, we want only the large airports (or those designated as such)\n", "and we don't need all the extra columns.\n", "\n", "Load your `all_us_airports.csv` and do some filtering as such:\n", "\n", "*  produce a new reference file with just the airports\n", "   of `large_airport` type, and \n", "    * there should only be 67\n", "    * name the file  `large_us_airports.csv`\n", "    * filter this data to only include the following relevant columns:\n", "      `name`, `latitude_deg`, `longitude_deg`, `iata_code`\n", "\n", "\n", "**&#167; Task:**  **Plot the data using Folium**\n", "\n", "The interactive capabilities of Jupyter are one of the main\n", "reasons we use it and you will now see why!\n", "\n", "We will use a library called `folium` to display an\n", "interactive map of all the airports in our `large_us_airports.csv`\n", "file.\n", "\n", "You should learn about folium here:\n", "\n", "* [https://python-visualization.github.io/folium/latest/](https://python-visualization.github.io/folium/latest/)\n", "\n", "In this part, you will create a map of the airports from the previous part.\n", "\n", "Do not overthink this -- it will be a few lines\n", "of code to load the data, loop over it, pull the lat/lon\n", "of each airport and then display the map.  PLay with the\n", "provided demos in the folium documentation.\n", "\n", "Your map should include the airport icon in the pin (Font Awesome \"airport\" icon)\n", "and the contents of the pin should be the airport\n", "name and iata code in parethesis, for example, `Denver\n", "International Airport (DEN)`.\n", "\n", "![](./sample_map.png)\n", "\n", "You can find out how to put a different icon in the pin\n", "from this documentation: folium [Icon documentation](https://python-visualization.github.io/folium/latest/user_guide/ui_elements/icons.html) \n", "\n", "\n", "\n", "### (25%) Perform  data engineering in Python using OpenAQ API. \n", "\n", "In this part, will do a small subset of the work\n", "required to build the dataset for the next part.\n", "\n", "We will use the OpenAQ data portal to obtain\n", "a dataset for later visualization and analysis.\n", "\n", "You will not need an API key for the OpenAQ\n", "API.  The service is FREE, but I will ask\n", "that you put a 1-2 second pause (what I call a _be nice_ pause)\n", "between each call.  While the API and service\n", "is FREE, running it is not.\n", "\n", "Learn more about OpenAQ here:\n", "\n", "- [https://openaq.org](https://openaq.org)\n", "\n", "They aggregate _global_ air quality data and need \n", "continued support to keep operations running smooth and to\n", "bring this amazing service to anyone on the planet Earth with\n", "an Internet connection.\n", "\n", "Find out how to make a donation to support\n", "their server, data storage and development\n", "costs here:\n", "\n", "- [https://secure.givelively.org/donate/openaq-inc/](https://secure.givelively.org/donate/openaq-inc/)\n", "\n", "**&#167; Task:**  **Obtain a dataset from OpenAQ**\n", "\n", "You will call OpenAQ API for the follow:\n", "\n", "1. obtain PM2.5 data only\n", "1. obtain a single day of data for June 6, 2023\n", "1. the data will be for all sensors within 7.5km\n", "  of downtown Detroit \n", "  * the lat/lon for downtown Detroit is: **42.33143000,-83.04575000**\n", "\n", "You will need to make sure you understand the documentation\n", "at OpenAQ.  Here is the best starting point to obtain\n", "data from the endpoint:\n", "\n", "* [https://openaq.org/developers/platform-overview/](https://openaq.org/developers/platform-overview/)\n", "* [https://docs.openaq.org/docs/introduction](https://docs.openaq.org/docs/introduction)\n", "\n", "This will provide information about the specific call to \n", "get the data:\n", "\n", "* `/measurements` endpoint: [https://docs.openaq.org/reference/measurements_get_v2_measurements_get](https://docs.openaq.org/reference/measurements_get_v2_measurements_get)\n", "\n", "\n", "**&#167; Task:**  **Transform, filter and store the OpenAQ data as CSV**\n", "\n", "After you've obtained the data, you will need make the data a little more \n", "useful as a single file.  The JSON is very valuable, but let's assume\n", "we do not need all of it (as is often the case).  So we're going\n", "to filter and restrict the data to just what we think we might need later.\n", "\n", "Beware, assuming what you \"need later\" can be tricky and it is always\n", "a good idea to keep the original data payloads (the JSON directly from\n", "OpenAQ) as a receipt of how your filtered data was brewed.\n", "\n", "Once you pull the data into a DataFrame, you'll notice a lot\n", "of data are actually JSON (unless you use the more advanced\n", "functions of `read_json`).  You will also notice there are fields\n", "we might likely not need (i.e. `isAnalysis`, `isMobile`, etc),\n", "again with the caveat that we don't really _know_ what we don't\n", "need, but we have some ideas about what we _do_ need.\n", "\n", "**Transform**\n", "\n", "1. convert the `coordinates` field to two fields: `sensor_lat` and `sensor_lon` which break\n", "  out the `coordinates->latitude` and `coordinates->longitude` correspondingly \n", "  * you will find [`DataFrame.apply()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html) to be exceedingly useful for this\n", "1. convert the `date` field to a single value using the `date->local` and make sure that value \n", "  is a `datetime64`.\n", "  * you will need to carefully study [`pd.to_datetime()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) documentation \n", "  * the new date field will be called `local_time`, once converted, you will not need the original `date` field\n", "\n", "**Filter**\n", "\n", "1. reduce the DataFrame to include only the fields: \n", "  `locationId`, `location`, `entity`, `parameter`, `value`, `sensor_lat`, and `sensor_lon`, `local_time`\n", "1. restrict manufacturer to the subset: `Governmental Organization` and `Community Organization`\n", "\n", "**Store**\n", "\n", "1. with the final transformed and filtered DataFrame:\n", "  * store it to a file called `20230606_detroit_downtown_7_5km_aq.csv`\n", "  * the file should have the columns `locationId`, `location`, `entity`, `parameter`, `value`, `local_time`, `sensor_lat`, and `sensor_lon`\n", "  * the file will have over six thousand lines in it\n", "\n", "\n", "\n", "### (20%) Perfom basic data analysis in Python using OpenAQ weather data. \n", "\n", "Now that we have some sample data (stations within 7.5km of downtown Detroit on June 6, 2023)\n", "we are going to look at air quality on that day using \n", "the core statistical tools we have learned about thus far.\n", "\n", "You may remember from the news last year, there was \n", "a [significant wildfire in Canada](https://archive.ph/20230607200733/https://www.bloomberg.com/news/articles/2023-06-07/hundreds-of-fires-are-out-of-control-in-canada-s-worst-ever-season#selection-298.1-4246.0) which produced a \n", "long-lasting plume of smoke over centeral and eastern North America \n", "and the United States for some time June.\n", "\n", "This fire rose to importance because it underscored\n", "the significance of smoke and particulate pollution on respiratory\n", "health, raising awareness about the importance of \n", "air quality monitoring and perhaps exposing some\n", "of the inadequacies of such monitoring not only in\n", "our country, but globally.\n", "\n", "Because OpenAQ has many (hudreds of) millions of data points, including\n", "those from PM2.5 air quality sensors, air quality around the time\n", "of this fire was being carefully monitored.  This data \n", "collection and open platform is the reason we can\n", "explore it further in this assignment.\n", "\n", "We will also notice that the sensors in OpenAQ data\n", "include government \"reference\" sensors, like those\n", "used and managed by the US EPA (Environmental Protection\n", "Agengy) for official data, as well as those providing \n", "data from \"community\" sensors made by companies like Purple Air.\n", "\n", "It will be noted, we are restricting our view to this short\n", "window of time to keep the computational burden on the \n", "cloud servers lower.  A larger dataset  would help\n", "provide more robust confidence of our analysis\n", "in this the assignment.\n", "\n", "**&#167; Task:**  **Load your Detroit data and answer the following questions**\n", "\n", "1. What is the mean and median PM2.5 reading over all sensors?\n", "2. What is the standard deviation?\n", "3. Which `location_id` recorded the highest PM2.5?  What was the reading?\n", "6. What is the ratio of `Community Organization` to `Governmental Organization` entity type?\n", "7. How many unique sensor stations are in the data (use `sensorId`)?\n", "8. What is the station density per km?\n", "8. What is the daily mean, median, min, max, 75% and standard deviation separately for all `Community Organization` \n", "and `Government Organization` sensors in the data (that is group each separately and report the statistics\n", "being asked)?\n", "8. What is your opinion of the differences in the statistics?  Comment specifically about the mean and 75%. \n", "\n", "\n", "**&#167; Task:**  **Build a map of the stations**\n", "\n", "Use the folium library to build the map:\n", "\n", "1. Put the pin for downtown Detroit in default blue with the \"city\" icon\n", "   * you will use [Font Awesome](https://fontawesome.com/icons?d=gallery) and\n", "    add the parameter `prefix=fa` to the build your icon, see\n", "    the folium [icon documentation](https://python-visualization.github.io/folium/latest/user_guide/ui_elements/icons.html) \n", "    for more information\n", "2. Put the community sensors on the map with green pins and the default info (\"i\") icon\n", "2. Put the government sensors on the map with red pins and the default asterisk (\"*\") icon\n", "\n", "Your map should look something like this:\n", "\n", "![](./detroit_map.png)\n", "\n", "\n", "**&#167; Task:**  **Explore hourly averages for the day**\n", "\n", "1. What are the average readings for each of the 6 hour blocks 12am-6am, 6am-noon, noon-6pm and 6pm-11:59pm?\n", "2. Compare and contrast these readings -- make a comment about their differences.  \n", "3. Plot the hourly averages for the day using line plots (see [`DataFrame.plot()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)).\n", "   - label the plot \"Hourly PM2.5 Averages for June 6, 2023 (Detroit, MI)\"\n", "   - the $x$-axis should be the hour (0..23)\n", "   - the $y$-axis should be the PM2.5 value\n", "4. Plot the hourly averages of the _government_ and _community_ sensors on the same plot.  The government\n", "   averages will be in blue, community in orange, the $x$ and $y$ axes the same as the previous plot.\n", "\n", "\n", "\n", "### (25%) Perfom basic statistical significance tests of air quality data. \n", "\n", "We know that the government sensors are intended to be ground truth and \n", "thus if the community sensors' values are similar to the government\n", "sensors, then we can have more confidence that their data can be trusted.\n", "\n", "One way to do this is, if given a data sample from each sensor,\n", "to use statistical tests to compare the data and determine if\n", "the data from each sensor are statistically\n", "\"the same\".  We do not intent that they are the actual\n", "same value, but that they are _statistically_ the same -- that their\n", "differences are not _meaningful_ or \"significant\" or \"statistically\n", "significant\".\n", "\n", "We will not delve into the depths of statistics here, other \n", "than to say that if two data samples can be compared \n", "and we want to know, perhaps, if the same sensor\n", "produced the data, with the assumption\n", "that the sensors were calibrated and deemed in working order.\n", "\n", "The goal then of a statistical test would be, perhaps,\n", "to take the means of the two samples, compare them\n", "_statistically_ and determine if there is a significant\n", "probability that the means of the two groups are \n", "_statistically_ the \"same\".  \n", "\n", "One such test we will use is called the t-test (aka \"Students t-test\").\n", "This test takes two data samples, statistically compares\n", "them and gives us another statistic and a p-value that\n", "give us a probability that the statistic is equal to or\n", "more extreme than the sample due to random chance alone.  You\n", "may have heard of the \"null hypothesis\", which states\n", "that there is no difference between the two sample statistics \n", "(e.g. sample means).  If\n", "we _reject_ or more appropriately _fail to accept_ \n", "the null hypothesis, we say that we have \n", "confidence the sample differences\n", "are not due to chance alone and that there _may_ be a true \n", "difference, which could be explained by something other than\n", "chance.\n", "\n", "The p-value puts a probability on that chance, and while\n", "p-values have undergone a lot of controversy in recent decades, they \n", "still relay useful information when stated properly.  They can put us in the ballpark\n", "of meaningful statistical analyses, but they often are misused\n", "to mean that _rejection of null hypothesis_ means **acceptance**\n", "that the alternative hypothesis is true, which is not at all the case!\n", "\n", "There is an excellent library in Python called [`scipy`](https://docs.scipy.org/) which \n", "provides many statistical tools for use in cases where Pandas or\n", "SciKit-Learn do not.  SciPy provides an\n", "independent t-test which we can use to determine if \n", "the means of these two\n", "sensors for this day are the same or if their differences\n", "are statistically significant. \n", "\n", "You can learn more about t-tests for significance:\n", "\n", "- [https://www.ncbi.nlm.nih.gov/books/NBK553048/](https://www.ncbi.nlm.nih.gov/books/NBK553048/)\n", "- [https://www.stat.cmu.edu/~hseltman/309/Book/chapter6.pdf](https://www.stat.cmu.edu/~hseltman/309/Book/chapter6.pdf)\n", "- [https://bookdown.org/introrbook/intro2r/t-test.html](https://bookdown.org/introrbook/intro2r/t-test.html)\n", "- [https://stats.libretexts.org/Bookshelves/Applied_Statistics/Biological_Statistics_(McDonald)](https://stats.libretexts.org/Bookshelves/Applied_Statistics/Biological_Statistics_(McDonald)/04%3A_Tests_for_One_Measurement_Variable/4.02%3A_Two-Sample_t-Test)\n", "\n", "What our aim will be is to build the case that the non-government \n", "sensors appear to be producing values as good as the government one's.\n", "\n", "But wait! We _assume_ that the government sensors are ground truth and \n", "thus correct.  We assume that the community sensors are calibrated and \n", "can produce values as good as the government one's.  What if these assumptions are unfounded?  During this analysis, you will see if\n", "these and other assumptions may require further investigation.\n", "\n", "**&#167; Task:**  **Determine if the sensor means for the day are different, and if that difference is _statistically significant_.**\n", "\n", "Before we get started, you will already realize there are many more data points\n", "across community sensors than government sensors.  This imbalance\n", "must be taken into account.  You will only have 95 data points for the government\n", "sensors and many thousands of data points from non-government (community) sensors.  \n", "\n", "1. In this task, first build a dataset with 95 data points sampled from 100 random\n", "draws of data from the community sensors:  \n", "\n", "  - an easy way to do this is with the `DataFrame.sample()`, with `95` as the parameter\n", "  - you can then loop 100 times and average over all those loops\n", "  - you may want to just concatenate the 100 draws to a 100 column by 95 row DataFrame and compute the `mean()`, but there are other ways\n", "\n", "2. What are the descriptive statistics of your sample and the government data?\n", "3. Compare and contrast, bring attention to the mean, 75% and standard deviations.\n", "4. Run a test for normality on the two samples using [`scipy.stats.normaltest()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html).\n", "  - Can you _fail to reject the null hypothesis_ (that the samples are drawn from normal distributions) at $\\alpha = 0.05$?\n", "  - You will only need to look at the p-value\n", "5. Run a [Barlett test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bartlett.html#scipy.stats.bartlett) \n", "  for equal variances (also known as _homoscedasticity_). You will only need to look at the p-value.\n", "  - Can you _fail to reject the null hypothesis_ (that the samples have equal variances) at $\\alpha = 0.05$?\n", "6. Run the [independent t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#scipy.stats.ttest_ind),\n", "  and use the result in the parameter `equal_var=`.  That is if you fail to reject the null hypothesis from Bartlett's\n", "  then `equal_var=True`.\n", "  - Can you _fail to reject the null hypothesis_ (that the means of two independent samples have identical\n", "   average expected values) at $\\alpha = 0.001$?  (Notice we have raised the bar for statistical significance!)\n", "   - Make sure your notebook emits the p-value of the t-test?\n", "\n", "\n", "**&#167; Task:**  **Reflect on the Detroit air quaity data of June 6, 2023 from each of the sensor types in the data**\n", "\n", "Going back to the original datasets (including all data points for community sensors), comment on the following:\n", "\n", "* What is your reaction to the statement: _Community sensors detected the poor air quality of the 2023 Canadian wildfires better than Government sensors_.\n", "  - use evidence to support your reaction\n", "  - take into account things like density in your reaction\n", "* What is your reaction to the statement: _There need to be more community sensors deployed in downtown Detroit_.\n", "  - use data and evidence in your reaction\n", "  - make note of other data points you might need to more thoughtfully react to the statement\n", "\n", "\n", "\n", "### (20%) Complete the online HW1 assessment. \n", "\n", "\n", "Once you are done with the coding part of the assignment, you will need to \n", "complete the online assessment for\n", "the final **4 points of your grade** for this assignment.\n", "\n", "**&#167; Task:**  Go to the course Blackboard and complete the assessment.\n", "\n", "\n", "\n", "### (5 extra points) BONUS: Build and analyze a more complex OpenAQ data set. \n", "\n", "You can earn up to 5 points extra for this part of the assignment.\n", "\n", "**&#167; Task:**  **Build the OpenAQ dataset for 2023 for JFK airport in New York City**\n", "\n", " - stations must be with 7.5km of the airport\n", " - date ranges must be May 1, 2023 to August 31, 2023\n", "\n", "\n", "**&#167; Task:**  **Analyze the data and answer the questions**\n", "\n", " - compare June 6 in your dataset with the Detroit dataset \n", " - comment on their similarities and differences both in terms\n", "  of sensor density and intensity of PM2.5 on that day\n", "\n", "\n", "\n"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [default]", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "12px", "width": "252px"}, "navigate_menu": true, "number_sections": false, "sideBar": true, "threshold": "1", "toc_cell": false, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 0}